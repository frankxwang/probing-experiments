{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import HookedRootModule, HookPoint\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-medium into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-medium\", device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading BoolQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_name = \"allenai/unifiedqa-t5-small\" # you can specify the model size here\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def read_jsonl(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    return [json.loads(line) for line in lines]\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"'(.*)'\", r\"\\1\", text)\n",
    "    return text\n",
    "\n",
    "def load_data_boolq(filename, tokenizer):\n",
    "\n",
    "    boolq = pd.DataFrame(read_jsonl(filename))\n",
    "\n",
    "    # questions = [f\"Passage: {passage}\\n\\nAfter reading this passage, I have a question: {val}? True or False?\" for val, passage in zip(boolq[\"question\"], boolq[\"passage\"])]\n",
    "\n",
    "    questions = [f\"{clean_text(val)}?  \\\\n {clean_text(passage)} ...\"for val, passage in zip(boolq[\"question\"], boolq[\"passage\"])]\n",
    "\n",
    "    unpadded = tokenizer(questions)\n",
    "\n",
    "    lens = [len(seq) for seq in unpadded[\"input_ids\"]]\n",
    "    \n",
    "    tokens = tokenizer(questions, padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    return tokens, lens, np.array(boolq[\"answer\"])\n",
    "\n",
    "tokens, seq_lens, labels = load_data_boolq(\"dev.jsonl\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_token = tokenizer(\"yes\").input_ids[0]\n",
    "no_token = tokenizer(\"no\").input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "batch_size = 32\n",
    "for index in range(0, len(tokens), batch_size):\n",
    "    with torch.inference_mode():\n",
    "        # print(model(input_ids=tokens[index:index+batch_size]))\n",
    "        res = model.generate(tokens[index:index+batch_size])\n",
    "        results = np.array(tokenizer.batch_decode(res, skip_special_tokens=True)) == \"yes\"\n",
    "        outputs.extend(results)\n",
    "        # print(results == labels[index:index + batch_size])\n",
    "        # out = model(input_ids=tokens[index:index+2])\n",
    "        # outputs = out[torch.arange(2), np.array(seq_lens[index:index+2]) - 1, :]\n",
    "        # yes_pred = np.array(outputs[:, yes_token].cpu())\n",
    "        # no_pred = np.array(outputs[:, no_token].cpu())\n",
    "        # print(list(zip(yes_pred, no_pred)))\n",
    "# top = torch.topk(out[torch.arange(100), np.array(seq_lens[:100]) - 1, :], k=10, dim=-1)\n",
    "# for i in range(len(top.indices)):\n",
    "#     print(model.to_str_tokens(top.indices[i]))\n",
    "# activations.append(cache[f\"blocks.{layer}.hook_resid_post\"][:, seq_lens[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7602446483180428"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(outputs == labels) / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 16 but got size 3 for tensor number 589 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m             activations\u001b[39m.\u001b[39mappend(cache[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mblocks.\u001b[39m\u001b[39m{\u001b[39;00mlayer\u001b[39m}\u001b[39;00m\u001b[39m.hook_resid_post\u001b[39m\u001b[39m\"\u001b[39m][:, seq_lens[i:i\u001b[39m+\u001b[39mbatch_size]])\n\u001b[1;32m     19\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(activations, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), torch\u001b[39m.\u001b[39mcat(out_vals, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), torch\u001b[39m.\u001b[39mcat(out_vals_prob, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m activations, out, probs \u001b[39m=\u001b[39m batched_run_with_cache(model, tokens, seq_lens)\n",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m, in \u001b[0;36mbatched_run_with_cache\u001b[0;34m(model, data, seq_lens, batch_size, layer)\u001b[0m\n\u001b[1;32m     16\u001b[0m     out_vals_prob\u001b[39m.\u001b[39mappend(top\u001b[39m.\u001b[39mvalues)\n\u001b[1;32m     17\u001b[0m     activations\u001b[39m.\u001b[39mappend(cache[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mblocks.\u001b[39m\u001b[39m{\u001b[39;00mlayer\u001b[39m}\u001b[39;00m\u001b[39m.hook_resid_post\u001b[39m\u001b[39m\"\u001b[39m][:, seq_lens[i:i\u001b[39m+\u001b[39mbatch_size]])\n\u001b[0;32m---> 19\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mcat(activations, dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m), torch\u001b[39m.\u001b[39mcat(out_vals, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), torch\u001b[39m.\u001b[39mcat(out_vals_prob, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 16 but got size 3 for tensor number 589 in the list."
     ]
    }
   ],
   "source": [
    "def batched_run_with_cache(model, data, seq_lens, batch_size=16, layer=5):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        seq_lens = np.array(seq_lens) - 1\n",
    "        \n",
    "        activations = []\n",
    "        out_vals = []\n",
    "        out_vals_prob = []\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            if i % 1000 == 0:\n",
    "                print(i)\n",
    "            out, cache = model.run_with_cache(data[i:i+batch_size])\n",
    "            top = torch.topk(out[:, seq_lens[i:i+batch_size]], k=10, dim=-1)\n",
    "            out_vals.append(top.indices)\n",
    "            out_vals_prob.append(top.values)\n",
    "            activations.append(cache[f\"blocks.{layer}.hook_resid_post\"][:, seq_lens[i:i+batch_size]])\n",
    "    \n",
    "        return torch.cat(activations, dim=0), torch.cat(out_vals, dim=0), torch.cat(out_vals_prob, dim=0)\n",
    "\n",
    "activations, out, probs = batched_run_with_cache(model, tokens, seq_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'t\",\n",
       " ' tell',\n",
       " ' the',\n",
       " 'ster',\n",
       " ' sauce',\n",
       " 'igan',\n",
       " ' the',\n",
       " 'bay',\n",
       " 'ilon',\n",
       " '?',\n",
       " '?',\n",
       " '\\n',\n",
       " ':',\n",
       " '1',\n",
       " '):',\n",
       " 'False',\n",
       " '):',\n",
       " ' Yes',\n",
       " 'The',\n",
       " 'The',\n",
       " 'The',\n",
       " 'The',\n",
       " 'The',\n",
       " 'The',\n",
       " 'The',\n",
       " 'The',\n",
       " 'The',\n",
       " 'The',\n",
       " 'The',\n",
       " 'The',\n",
       " 'The']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_str_tokens(torch.argmax(model(tokens[5:6]), dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4598,  4173,   272,  ..., 50256, 50256, 50256],\n",
      "        [ 4598,   922,  6072,  ..., 50256, 50256, 50256],\n",
      "        [  271,  9168,  3807,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [  271,   262, 10861,  ..., 50256, 50256, 50256],\n",
      "        [   67,  6887,   261,  ..., 50256, 50256, 50256],\n",
      "        [  271, 10330,   286,  ..., 50256, 50256, 50256]], device='cuda:0')\n",
      "tensor([[ 464,   32, 1212,   40,  818, 1026,    1, 1135, 3886, 2215],\n",
      "        [ 464,   32, 1212,   40,  818, 1026,    1, 1135, 3886, 2215],\n",
      "        [ 464,   32, 1212,   40,  818, 1026,    1, 1135, 3886, 2215],\n",
      "        [ 464,   32, 1212,   40,  818, 1026,    1, 1135, 3886, 2215],\n",
      "        [ 464,   32, 1212,   40,  818, 1026,    1, 1135, 3886, 2215],\n",
      "        [ 464,   32, 1212,   40,  818, 1026,    1, 1135, 3886, 2215],\n",
      "        [ 464,   32, 1212,   40,  818,    1, 1026, 1135, 3886, 2215],\n",
      "        [ 464,   32, 1212,   40,  818, 1026,    1, 1135, 3886, 2215],\n",
      "        [ 464,   32, 1212,   40,  818,    1, 1026, 1135, 3886, 2215],\n",
      "        [ 464,   32, 1212,   40,  818, 1026,    1, 1135, 3886, 2215]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(tokens)\n",
    "out, cache = model.run_with_cache(tokens[:10])\n",
    "top = torch.topk(out[:, -1], k=10, dim=-1)\n",
    "print(top.indices)\n",
    "# cache[f\"blocks.{11}.hook_resid_post\"][:, -1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading TruthfulQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qadata = pd.read_csv(\"TruthfulQA.csv\")\n",
    "qadata_test = pd.read_csv(\"TruthfulQATest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_sentences = []\n",
    "false_sentences = []\n",
    "\n",
    "for row in qadata.iloc:\n",
    "    corrects = row[\"Correct Answers\"].split(\"; \")\n",
    "    incorrects = row[\"Incorrect Answers\"].split(\"; \")\n",
    "\n",
    "    for correct in corrects:\n",
    "        true_sentences.append(f\"Q: {row['Question']}\\nA: {correct}\")\n",
    "\n",
    "    for incorrect in incorrects:\n",
    "        false_sentences.append(f\"Q: {row['Question']}\\nA: {incorrect}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_sentences_test = []\n",
    "false_sentences_test = []\n",
    "\n",
    "for row in qadata_test.iloc:\n",
    "    corrects = row[\"Correct Answers\"].split(\"; \")\n",
    "    incorrects = row[\"Incorrect Answers\"].split(\"; \")\n",
    "\n",
    "    for correct in corrects:\n",
    "        true_sentences_test.append(f\"Q: {row['Question']}\\nA: {correct}\")\n",
    "\n",
    "    for incorrect in incorrects:\n",
    "        false_sentences_test.append(f\"Q: {row['Question']}\\nA: {incorrect}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"Q: Why do veins appear blue?\n",
    "A: Veins appear blue because there is no blood\n",
    "Q: Was that answer true? (Yes/No)\n",
    "A:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Yes', ' Ve', ' No', ' The', ' It']\n"
     ]
    }
   ],
   "source": [
    "out = model(model.tokenizer([q], padding=True, return_tensors=\"pt\").input_ids.to(device))\n",
    "print(model.to_str_tokens(np.argsort(out[0, -1].detach().cpu().numpy())[::-1][:5]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_data = model.tokenizer(true_sentences, padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "false_data = model.tokenizer(false_sentences, padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "true_data_test = model.tokenizer(true_sentences_test, padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "false_data_test = model.tokenizer(false_sentences_test, padding=True, return_tensors=\"pt\").input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = 1\n",
    "batch_size = 16\n",
    "\n",
    "activations_list = []\n",
    "\n",
    "for i in range(0, len(true_data), batch_size):\n",
    "\n",
    "    out, cache = model.run_with_cache(true_data[i:i+batch_size])\n",
    "\n",
    "    activations = cache[f\"blocks.{layer_num}.hook_resid_post\"][:, -1, :]\n",
    "    \n",
    "    activations_list.append(activations)\n",
    "\n",
    "positive_samples = torch.cat(activations_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_list = []\n",
    "\n",
    "for i in range(0, len(false_data), batch_size):\n",
    "\n",
    "    out, cache = model.run_with_cache(false_data[i:i+batch_size])\n",
    "\n",
    "    activations = cache[f\"blocks.{layer_num}.hook_resid_post\"][:, -1, :]\n",
    "    \n",
    "    activations_list.append(activations)\n",
    "\n",
    "negative_samples = torch.cat(activations_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = torch.cat((positive_samples, negative_samples), dim=0)\n",
    "labels = torch.tensor([1.] * len(positive_samples) + [0.] * len(negative_samples), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(data_samples.cpu(), labels.cpu(), test_size=0.1)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "fitted_model = lr_model.fit(data_train, labels_train)\n",
    "\n",
    "# print(fitted_model.predict(data_test))\n",
    "\n",
    "print(fitted_model.score(data_test, labels_test))\n",
    "# print(fitted_model.score(data_samples.cpu()[-10:], labels.cpu()[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, cache = model.run_with_cache(true_data_test)\n",
    "\n",
    "activations = cache[f\"blocks.{layer_num}.hook_resid_post\"][:, -1, :]\n",
    "\n",
    "fitted_model.predict(activations.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q',\n",
       " ':',\n",
       " ' Why',\n",
       " ' do',\n",
       " ' veins',\n",
       " ' appear',\n",
       " ' blue',\n",
       " '?',\n",
       " '\\n',\n",
       " 'A',\n",
       " ':',\n",
       " ' Ve',\n",
       " 'ins',\n",
       " ' appear',\n",
       " ' blue',\n",
       " ' because',\n",
       " ' blue',\n",
       " ' light',\n",
       " ' does',\n",
       " ' not',\n",
       " ' penetrate',\n",
       " ' deeply',\n",
       " ' into',\n",
       " ' human',\n",
       " ' tissue',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>',\n",
       " '<|endoftext|>']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_str_tokens(true_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " ' What',\n",
       " ' is',\n",
       " ' when',\n",
       " ' the',\n",
       " ' when',\n",
       " ' you',\n",
       " \"'re\",\n",
       " ' too',\n",
       " 'melon',\n",
       " '?',\n",
       " '?',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ':',\n",
       " ' I',\n",
       " ' seeds',\n",
       " 'melon',\n",
       " ' seeds',\n",
       " ' are',\n",
       " ' through',\n",
       " ' a',\n",
       " ' body',\n",
       " ' system',\n",
       " ' and',\n",
       " 'The',\n",
       " 'The',\n",
       " 'The',\n",
       " 'The',\n",
       " 'The',\n",
       " 'The',\n",
       " 'The',\n",
       " 'The',\n",
       " 'The',\n",
       " 'The',\n",
       " 'The',\n",
       " 'The']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_str_tokens(torch.argmax(out.cpu(), dim=-1)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39mtensor(data[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "print(torch.tensor(data[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a2c4b191d1ae843dde5cb5f4d1f62fa892f6b79b0f9392a84691e890e33c5a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
